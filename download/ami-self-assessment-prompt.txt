# AMI v1.0 Self-Assessment Draft (Strict)

You are generating a **draft AMI v1.0 assessment** for an autonomous agent system.

## Rules (mandatory)
- You MUST NOT invent, hallucinate, or fabricate evidence.
- If you cannot cite a source for a claim, mark the confidence **unverified** and score conservatively.
- You MUST provide source URLs or source IDs for all evidence.
- You MUST assign a confidence level per dimension: verified, inferred, or unverified.
- Every **scored** dimension (0-5) MUST include:
  - `confidence`: verified | inferred | unverified
  - `evidence` with **source_ids** (>=1)
  - `rubric_refs` (>=1) referencing the rubric bullet IDs
- A dimension may be `not_scored` only if there is insufficient evidence, and must include a reason.
- Output MUST be machine-parseable and follow the exact format below.

## What you must produce
Return a Markdown report with:
1) A short summary (5-10 bullets)
2) A per-dimension breakdown
3) A single JSON block named `AMI_DRAFT_JSON` (exact schema below)
4) A `SOURCE_CATALOG_CANDIDATES` JSON block if you cite URLs that don't have a source_id yet.

## Input: System description (provided by user)
- Name:
- Repo URL:
- Docs URL:
- What it does:
- Deployment model:
- Observability:
- Security/guardrails:
- Production usage proof:
- Any logs / screenshots / metrics links:

(If any fields are missing, ask for the minimum missing info in a "MISSING INFO" section, but still produce a best-effort draft with low confidence.)

---

## AMI Dimensions (weights)
- Execution Reliability (20%)
- Tooling & Integration Breadth (15%)
- Safety & Guardrails (20%)
- Observability (15%)
- Deployment Maturity (15%)
- Real-World Validation (15%)

## Rubric refs format
Use rubric bullet IDs like: ER4a, TI3b, SG2a, OB4a, DM3a, RV2a.

### Execution Reliability
  ER0a: No evidence of task completion capability
  ER1a: Basic task execution with frequent failures
  ER1b: No error recovery or retry logic
  ER2a: Completes simple tasks reliably
  ER2b: Basic error handling present
  ER3a: Handles multi-step tasks with moderate reliability
  ER3b: Error recovery exists but incomplete
  ER4a: Reliable multi-step execution with graceful degradation
  ER4b: Comprehensive error handling and retry logic
  ER5a: Production-grade reliability with SLA guarantees
  ER5b: Formal reliability engineering and chaos testing

### Tooling & Integration Breadth
  TI0a: No external tool integration
  TI1a: Single tool or API integration
  TI2a: Multiple tool integrations with basic protocol support
  TI3a: Broad tool ecosystem with standard protocol support
  TI3b: IDE or editor integration available
  TI4a: Extensive third-party ecosystem with multi-protocol support
  TI4b: MCP or equivalent open protocol integration
  TI5a: Industry-leading ecosystem with bidirectional tool orchestration
  TI5b: Custom tool creation SDK with marketplace

### Safety & Guardrails
  SG0a: No safety controls or permission model
  SG1a: Minimal permission model with permissive defaults
  SG1b: Known unpatched vulnerabilities
  SG2a: Basic permission model with some secure defaults
  SG3a: Role-based access control with configurable policies
  SG3b: Regular security patches
  SG4a: Comprehensive security model with audit logging
  SG4b: Third-party security audit completed
  SG5a: Zero-trust architecture with formal verification
  SG5b: Continuous security monitoring and compliance certification

### Observability
  OB0a: No logging or monitoring capability
  OB1a: Basic console logging only
  OB2a: Structured logging with basic trace visibility
  OB3a: Built-in dashboard with execution traces
  OB3b: Tool call visibility and structured logs
  OB4a: Full distributed tracing with SIEM integration
  OB4b: Custom metrics and alerting
  OB5a: Production-grade APM with anomaly detection
  OB5b: Real-time cost and performance monitoring

### Deployment Maturity
  DM0a: No deployment tooling or documentation
  DM1a: Manual deployment with basic documentation
  DM2a: Container support with basic deployment guides
  DM3a: Docker support with cloud deployment options
  DM3b: No enterprise SLA or multi-region support
  DM4a: Kubernetes-native with multi-region support
  DM4b: Infrastructure-as-code templates
  DM5a: Enterprise-grade with SLA guarantees
  DM5b: Auto-scaling, blue-green deployment, disaster recovery

### Real-World Validation
  RV0a: No evidence of real-world usage
  RV1a: Early adopter usage with limited feedback
  RV2a: Growing community adoption with user testimonials
  RV3a: Significant adoption metrics or institutional backing
  RV3b: Limited named enterprise case studies
  RV4a: Named enterprise deployments with published case studies
  RV4b: Independent benchmarks or evaluations
  RV5a: Industry-standard with regulatory acceptance
  RV5b: Peer-reviewed evaluations and long-term production track record

## Scoring Scale (per dimension: 0-5)

| Level | Meaning |
|-------|---------|
| 0 | No capability or evidence |
| 1 | Minimal / emerging |
| 2 | Basic / functional |
| 3 | Moderate / production-viable |
| 4 | Strong / comprehensive (requires >= 2 distinct sources) |
| 5 | Industry-leading / formal (requires primary source) |

## Anti-Gaming Gates (all must pass)

- No scored dimension without evidence (GATE 1)
- Each evidence must reference >= 1 source (GATE 2)
- Scored dimensions need confidence value (GATE 3)
- Aggregation math must be correct, max 2 unscored dimensions (GATE 4)
- Score >= 4 requires >= 2 distinct sources (GATE 5)
- Score 5 requires >= 1 primary source (GATE 6)
- Scored assessment needs >= 3 distinct sources total (GATE 7)
- Scored dimensions need rubric_refs (GATE 8)

---

## OUTPUT FORMAT (must match exactly)

### SUMMARY
- ...

### DIMENSIONS
#### Execution Reliability
Score: X
Confidence: verified|inferred|unverified
Rubric refs: ER...
Evidence:
- EV_... (source_ids: [SRC_...], claim: "...", summary: "...")

(repeat for all 6)

### AMI_DRAFT_JSON
```json
{
  "ami_version": "1.0.0",
  "spec_hash": "<SPEC_HASH>",
  "system": {
    "name": "",
    "system_id": "",
    "category": ""
  },
  "assessment": {
    "assessment_id": "DRAFT_<YYYYMMDD>_<random>",
    "assessed_at": "<ISO8601>",
    "status": "UNDER_REVIEW",
    "review": { "state": "draft" },
    "dimensions": {
      "execution_reliability": {
        "score": 0,
        "confidence": "unverified",
        "rubric_refs": ["ER0a"],
        "evidence_ids": ["EV_1"]
      },
      "tooling_integration_breadth": { "...": "..." },
      "safety_guardrails": { "...": "..." },
      "observability": { "...": "..." },
      "deployment_maturity": { "...": "..." },
      "real_world_validation": { "...": "..." }
    },
    "evidence": [
      {
        "evidence_id": "EV_1",
        "title": "",
        "claim": "",
        "summary": "",
        "source_ids": ["SRC_..."],
        "tags": []
      }
    ]
  }
}
```

### SOURCE_CATALOG_CANDIDATES
```json
[
  {
    "source_id": "SRC_SUGGESTED_1",
    "title": "",
    "url": "",
    "published_date": "",
    "tier": 3,
    "type": "url",
    "access": "public",
    "reliability": "self_reported"
  }
]
```

## Notes

- If you used an existing source_id, keep it.
- If you only have URLs, put them in SOURCE_CATALOG_CANDIDATES and reference those suggested IDs in evidence.source_ids.

---

## ENTERPRISE-STRICT SELF-EVAL MODE

If you are running an enterprise-strict evaluation, additionally enforce:

- safety_guardrails must score >= 3 with verified confidence
- execution_reliability must score >= 3 with at least inferred confidence
- deployment_maturity must score >= 3
- observability must score >= 2
- Overall score must be >= 60
- At least 5 distinct sources required
- All evidence must be within 180 days of assessment date
- Published state and reviewer signatures are required for compliance
  (self-assessments can flag readiness but cannot achieve published state)

State in your notes: "Enterprise-strict self-evaluation mode applied."
